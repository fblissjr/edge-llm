# --- Global Server Settings ---
default_model: "gemma3n-e4b-it"
max_loaded_models: 2

# --- Model Definitions ---
models:
  # --- MLX Models ---
  - id: "gemma3n-e4b-it"
    provider: "mlx"
    description: "Gemma 3 4B Instruct with vision capabilities"
    tags: ["vision", "instruct", "gemma"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma-3n-E4B-it-bf16-mlx"
      vision: true

  - id: "gemma3-27b-dwq"
    provider: "mlx"
    description: "Gemma 3 27B with a 1B draft model and optimized caching"
    tags: ["large", "speculative", "fast", "text-only"]
    enabled: true
    config:
      model_path: "modelzoo/mlx-community/gemma3-27b-it-4bit-DWQ-mlx"
      draft_model_path: "modelzoo/mlx-community/gemma-3-1b-it-qat-4bit"
      num_draft_tokens: 6
      vision: true
      cache_type: "quantized"
      kv_bits: 4

  # --- GGUF Models via llama.cpp ---
  - id: "qwen2.5-vl-72b-inst-gguf"
    provider: "llama_cpp"
    description: "Qwen2.5-VL 72B Instruct quantized GGUF"
    tags: ["vision", "large", "gguf", "qwen"]
    enabled: true
    config:
      model_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-q4_0_l.gguf"
      mmproj_path: "modelzoo/Qwen/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-mmproj-f16.gguf"
      chat_format: "qwen"
      n_gpu_layers: -1
      n_ctx: 8192
      vision: true

  - id: "mistral-small-vision"
    provider: "llama_cpp"
    description: "Mistral Small 24B with vision capabilities and custom template"
    tags: ["vision", "mistral", "custom-template"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf"
      mmproj_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf"
      chat_format_template: "templates/Mistral-Small-3.2-24B-Instruct-2506.jinja2"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true

  - id: "Llama-3.2-1B-Instruct-4bit"
    provider: "mlx"
    description: "Llama-3.2-1B-Instruct-4bit"
    tags: ["small", "speculative", "fast", "text-only"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      cache_type: "quantized"
      kv_bits: 4

  - id: "llama-3.1-8b-instruct"
    provider: "llama_cpp"
    description: "Llama 3.1 8B Instruct - GGUF version"
    tags: ["text-only", "instruct", "llama"]
    enabled: true
    config:
      model_path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
      chat_format: "llama-3"
      n_gpu_layers: -1
      n_ctx: 8192
      vision: false
