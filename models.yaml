# Enhanced models.yaml configuration with advanced features
# Global server settings
default_model: "gemma3n-e4b-it" # Optional: specify default model
max_concurrent_requests: 2
request_timeout: 300
log_level: "DEBUG"

models:
  - id: "gemma3n-e4b-it"
    provider: "mlx"
    description: "Gemma 3 4B Instruct with vision capabilities"
    tags: ["vision", "instruct", "gemma"]
    enabled: true
    config:
      model_path: "modelzoo/google/gemma-3n-E4B-it-bf16-mlx"
      vision: true
      temperature: 1.0
      top_p: 0.95
      max_tokens: 2048

  # --- Large Model with Speculative Decoding ---
  - id: "gemma3-27b-dwq"
    provider: "mlx"
    description: "Gemma 3 27B with 1B draft model for faster inference"
    tags: ["large", "speculative", "fast"]
    enabled: true
    config:
      model_path: "modelzoo/mlx-community/gemma3-27b-it-4bit-DWQ-mlx"
      draft_model_path: "modelzoo/mlx-community/gemma-3-1b-it-qat-4bit"
      num_draft_tokens: 8 # More aggressive speculation
      vision: false
      temperature: 0.8
      top_p: 0.95
      max_tokens: 4096
      # Memory optimization for large model
      max_kv_size: 16384
      kv_bits: 4 # More aggressive quantization
      quantized_kv_start: 2000

  # --- GGUF Models via llama.cpp ---
  - id: "qwen2.5-vl-72b-inst-gguf"
    provider: "llama_cpp"
    description: "Qwen2.5-VL 72B Instruct quantized GGUF"
    tags: ["vision", "large", "gguf", "qwen"]
    enabled: true
    config:
      model_path: "modelzoo/Mungert/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-q4_0_l.gguf"
      mmproj_path: "modelzoo/Mungert/Qwen2.5-VL-72B-Instruct-GGUF/Qwen2.5-VL-72B-Instruct-mmproj-f16.gguf"
      chat_format: "qwen-vl"
      n_gpu_layers: -1 # Use all GPU layers
      n_ctx: 8192
      n_batch: 256 # Smaller batch for memory efficiency
      vision: true
      temperature: 0.7
      top_p: 0.8
      max_tokens: 2048

  - id: "mistral-small-vision"
    provider: "llama_cpp"
    description: "Mistral Small 24B with vision capabilities"
    tags: ["vision", "mistral", "custom-template"]
    enabled: true
    config:
      model_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf"
      mmproj_path: "modelzoo/mistral/Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-Mistral-Small-3.2-24B-Instruct-2506-F16.gguf"
      chat_format_template: "templates/Mistral-Small-3.2-24B-Instruct-2506.jinja2"
      eos_token: "</s>"
      bos_token: "<s>"
      n_gpu_layers: -1
      n_ctx: 4096
      vision: true
      temperature: 0.8
      repetition_penalty: 1.1

  # --- Draft Models (for speculative decoding) ---
  - id: "qwen2.5-vl-draft"
    provider: "llama_cpp"
    description: "Qwen2.5-VL 3B - Fast draft model"
    tags: ["draft", "fast", "qwen"]
    enabled: true
    config:
      model_path: "modelzoo/Mungert/Qwen2.5-VL-3B-Instruct-GGUF"
      chat_format: "qwen-vl"
      n_gpu_layers: -1
      n_ctx: 8192
      draft_model: true
      vision: true

  # --- Text-Only Models ---
  - id: "llama-3.1-8b-instruct"
    provider: "mlx"
    description: "Llama 3.1 8B Instruct - General purpose text model"
    tags: ["text-only", "instruct", "llama"]
    enabled: false # Disabled by default
    config:
      model_path: "modelzoo/mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"
      vision: false
      temperature: 0.9
      top_p: 0.95
      top_k: 50 # Add top-k sampling
      min_p: 0.05 # Add min-p filtering
      max_tokens: 4096
      repetition_penalty: 1.05
